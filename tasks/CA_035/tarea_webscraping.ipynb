{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tarea Web-scrapping 200035\n",
    "\n",
    "### Crea una carpeta en 'tasks' con el formato *INICIALES_3UltimosDigitosCU* y copia de este archivo dentro de ella. NO ALTERES ESTA VERSIÓN DEL DOCUMENTO.\n",
    "\n",
    "### Realiza `push` a tus cambios y haz el `pull request`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utiliza la siguiente página para scrapear: https://books.toscrape.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sel-docker\n",
      "sel-docker\n",
      "e4909ca0aaee5f149217413823a79d86a1b87e2249007ed723fb1a00ab793241\n"
     ]
    }
   ],
   "source": [
    "!docker stop sel-docker\n",
    "!docker rm sel-docker\n",
    "!docker run -d --name \"sel-docker\" -p 4444:4444 --shm-size=2g \\\n",
    "  -e SE_NODE_MAX_SESSIONS=6 \\\n",
    "  -e SE_NODE_SESSION_TIMEOUT=1200 \\\n",
    "  -e SE_VNC_NO_PASSWORD=1 \\\n",
    "  selenium/standalone-chrome\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "#instalar Beautiful Soup\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Inicializar el driver de Selenium\n",
    "driver = webdriver.Remote('http://localhost:4444/wd/hub',options=webdriver.ChromeOptions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abre la página"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Abrir la página\n",
    "url='https://books.toscrape.com/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraer el título de la página"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El título de la página es: All products | Books to Scrape - Sandbox\n"
     ]
    }
   ],
   "source": [
    "#Extrae el título de la página \n",
    "nom_pagina = driver.title #titulo de la página\n",
    "print(\"El título de la página es:\", nom_pagina)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Encontrar un elemento (puede ser un botón, un enlace, un campo de entrada, etc.) utilizando XPath, ID, u otra estrategia de localización de elementos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpathImagen=\"//li[@class='col-xs-6 col-sm-4 col-md-3 col-lg-3']/article[@class='product_pod']/div[@class='image_container']/a/img\"\n",
    "imagen = driver.find_elements(By.XPATH, xpathImagen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizar alguna acción con el elemento del markdown anterior (por ejemplo, hacer clic en el botón, introducir texto en un campo de entrada, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Realiza la acción\n",
    "# Haz clic en el botón\n",
    "#botton.click()\n",
    "imagen = wait.until(EC.presence_of_element_located((By.XPATH, xpathImagen)))\n",
    "\n",
    "imagen.click()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los scrappers son herramientas poderosas para extraer automáticamente datos de una página web. ChatGPT puede ser una ayuda valiosa para generar expresiones XPath que indican dónde se encuentran los datos que deseamos extraer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debemos hacer el inspect y buscar un fragmento de html que le pueda dar contexto a ChatGTP sobre como hacer la consulta, Inspecciona la página hasta encontrar el nodo: ```<li class=\"col-xs-6 col-sm-4 col-md-3 col-lg-3\">```, copia esta información y pide a chatGTP que te dé el XPATH de los títulos. Pruebalo haciendo ctrl+f en la herramienta de inspección"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                title\n",
      "0                                A Light in the Attic\n",
      "1                                  Tipping the Velvet\n",
      "2                                          Soumission\n",
      "3                                       Sharp Objects\n",
      "4               Sapiens: A Brief History of Humankind\n",
      "5                                     The Requiem Red\n",
      "6   The Dirty Little Secrets of Getting Your Dream...\n",
      "7   The Coming Woman: A Novel Based on the Life of...\n",
      "8   The Boys in the Boat: Nine Americans and Their...\n",
      "9                                     The Black Maria\n",
      "10     Starving Hearts (Triangular Trade Trilogy, #1)\n",
      "11                              Shakespeare's Sonnets\n",
      "12                                        Set Me Free\n",
      "13  Scott Pilgrim's Precious Little Life (Scott Pi...\n",
      "14                          Rip it Up and Start Again\n",
      "15  Our Band Could Be Your Life: Scenes from the A...\n",
      "16                                               Olio\n",
      "17  Mesaerion: The Best Science Fiction Stories 18...\n",
      "18                       Libertarianism for Beginners\n",
      "19                            It's Only the Himalayas\n"
     ]
    }
   ],
   "source": [
    "# Configurar XPath\n",
    "xpath_titulos = \"//li[@class='col-xs-6 col-sm-4 col-md-3 col-lg-3']/article[@class='product_pod']/h3/a\"\n",
    "\n",
    "# Esperar hasta que al menos un elemento con la clase especificada esté presente en el DOM\n",
    "wait.until(EC.presence_of_element_located((By.XPATH, xpath_titulos)))\n",
    "\n",
    "# Obtener los elementos de título después de que el elemento esté presente\n",
    "elementos_titulos = driver.find_elements(By.XPATH, xpath_titulos)\n",
    "\n",
    "# Obtener los títulos\n",
    "titulos = [element.get_attribute(\"title\") for element in elementos_titulos]\n",
    "\n",
    "# Crear DataFrame de Pandas\n",
    "data = {'title': titulos}\n",
    "df_title = pd.DataFrame(data)\n",
    "\n",
    "# Mostrar el DataFrame\n",
    "print(df_title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploraremos cómo ChatGPT puede generar XPATH para extraer precios y otros datos específicos de una página web. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  URL\n",
      "0   https://books.toscrape.com/catalogue/a-light-i...\n",
      "1   https://books.toscrape.com/catalogue/tipping-t...\n",
      "2   https://books.toscrape.com/catalogue/soumissio...\n",
      "3   https://books.toscrape.com/catalogue/sharp-obj...\n",
      "4   https://books.toscrape.com/catalogue/sapiens-a...\n",
      "5   https://books.toscrape.com/catalogue/the-requi...\n",
      "6   https://books.toscrape.com/catalogue/the-dirty...\n",
      "7   https://books.toscrape.com/catalogue/the-comin...\n",
      "8   https://books.toscrape.com/catalogue/the-boys-...\n",
      "9   https://books.toscrape.com/catalogue/the-black...\n",
      "10  https://books.toscrape.com/catalogue/starving-...\n",
      "11  https://books.toscrape.com/catalogue/shakespea...\n",
      "12  https://books.toscrape.com/catalogue/set-me-fr...\n",
      "13  https://books.toscrape.com/catalogue/scott-pil...\n",
      "14  https://books.toscrape.com/catalogue/rip-it-up...\n",
      "15  https://books.toscrape.com/catalogue/our-band-...\n",
      "16  https://books.toscrape.com/catalogue/olio_984/...\n",
      "17  https://books.toscrape.com/catalogue/mesaerion...\n",
      "18  https://books.toscrape.com/catalogue/libertari...\n",
      "19  https://books.toscrape.com/catalogue/its-only-...\n"
     ]
    }
   ],
   "source": [
    "xpath_url_de_libros=\"//li[@class='col-xs-6 col-sm-4 col-md-3 col-lg-3']/article[@class='product_pod']/div[@class='image_container']/a\"\n",
    "\n",
    "wait.until(EC.presence_of_element_located((By.XPATH, \"//li[@class='col-xs-6 col-sm-4 col-md-3 col-lg-3']/article[@class='product_pod']/div[@class='image_container']/a\")))\n",
    "\n",
    "# Obtener el atributo href después de que el elemento esté presente\n",
    "url_de_libros = [url.get_attribute(\"href\") for url in driver.find_elements(By.XPATH, \"//li[@class='col-xs-6 col-sm-4 col-md-3 col-lg-3']/article[@class='product_pod']/div[@class='image_container']/a\")]\n",
    "\n",
    "# Crear DataFrame de Pandas\n",
    "data = {'URL': url_de_libros}\n",
    "df_URL = pd.DataFrame(data)\n",
    "\n",
    "# Mostrar el DataFrame\n",
    "print(df_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Precio (£)\n",
      "0       51.77\n",
      "1       53.74\n",
      "2       50.10\n",
      "3       47.82\n",
      "4       54.23\n",
      "5       22.65\n",
      "6       33.34\n",
      "7       17.93\n",
      "8       22.60\n",
      "9       52.15\n",
      "10      13.99\n",
      "11      20.66\n",
      "12      17.46\n",
      "13      52.29\n",
      "14      35.02\n",
      "15      57.25\n",
      "16      23.88\n",
      "17      37.59\n",
      "18      51.33\n",
      "19      45.17\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Configurar XPath para los precios\n",
    "xpath_precios = \"//li[@class='col-xs-6 col-sm-4 col-md-3 col-lg-3']//p[@class='price_color']\"\n",
    "\n",
    "# Esperar hasta que al menos un elemento con la clase especificada esté presente en el DOM\n",
    "wait.until(EC.presence_of_element_located((By.XPATH, xpath_precios)))\n",
    "\n",
    "# Obtener los elementos de precio después de que el elemento esté presente\n",
    "elementos_precios = driver.find_elements(By.XPATH, xpath_precios)\n",
    "\n",
    "# Obtener los precios\n",
    "precios = [element.text.strip('£') for element in elementos_precios]\n",
    "\n",
    "# Crear DataFrame de Pandas\n",
    "data = {'Precio (£)': precios}\n",
    "df_precios = pd.DataFrame(data)\n",
    "\n",
    "# Mostrar el DataFrame\n",
    "print(df_precios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con la información obtenida, guardala en un DataFrame de pandas y encuentrar el libro más barato y el más caro de la primera página mostrada y brinda sus URL's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Precio (£)                                              title  \\\n",
      "0       51.77                               A Light in the Attic   \n",
      "1       53.74                                 Tipping the Velvet   \n",
      "2       50.10                                         Soumission   \n",
      "3       47.82                                      Sharp Objects   \n",
      "4       54.23              Sapiens: A Brief History of Humankind   \n",
      "5       22.65                                    The Requiem Red   \n",
      "6       33.34  The Dirty Little Secrets of Getting Your Dream...   \n",
      "7       17.93  The Coming Woman: A Novel Based on the Life of...   \n",
      "8       22.60  The Boys in the Boat: Nine Americans and Their...   \n",
      "9       52.15                                    The Black Maria   \n",
      "10      13.99     Starving Hearts (Triangular Trade Trilogy, #1)   \n",
      "11      20.66                              Shakespeare's Sonnets   \n",
      "12      17.46                                        Set Me Free   \n",
      "13      52.29  Scott Pilgrim's Precious Little Life (Scott Pi...   \n",
      "14      35.02                          Rip it Up and Start Again   \n",
      "15      57.25  Our Band Could Be Your Life: Scenes from the A...   \n",
      "16      23.88                                               Olio   \n",
      "17      37.59  Mesaerion: The Best Science Fiction Stories 18...   \n",
      "18      51.33                       Libertarianism for Beginners   \n",
      "19      45.17                            It's Only the Himalayas   \n",
      "\n",
      "                                                  URL  \n",
      "0   https://books.toscrape.com/catalogue/a-light-i...  \n",
      "1   https://books.toscrape.com/catalogue/tipping-t...  \n",
      "2   https://books.toscrape.com/catalogue/soumissio...  \n",
      "3   https://books.toscrape.com/catalogue/sharp-obj...  \n",
      "4   https://books.toscrape.com/catalogue/sapiens-a...  \n",
      "5   https://books.toscrape.com/catalogue/the-requi...  \n",
      "6   https://books.toscrape.com/catalogue/the-dirty...  \n",
      "7   https://books.toscrape.com/catalogue/the-comin...  \n",
      "8   https://books.toscrape.com/catalogue/the-boys-...  \n",
      "9   https://books.toscrape.com/catalogue/the-black...  \n",
      "10  https://books.toscrape.com/catalogue/starving-...  \n",
      "11  https://books.toscrape.com/catalogue/shakespea...  \n",
      "12  https://books.toscrape.com/catalogue/set-me-fr...  \n",
      "13  https://books.toscrape.com/catalogue/scott-pil...  \n",
      "14  https://books.toscrape.com/catalogue/rip-it-up...  \n",
      "15  https://books.toscrape.com/catalogue/our-band-...  \n",
      "16  https://books.toscrape.com/catalogue/olio_984/...  \n",
      "17  https://books.toscrape.com/catalogue/mesaerion...  \n",
      "18  https://books.toscrape.com/catalogue/libertari...  \n",
      "19  https://books.toscrape.com/catalogue/its-only-...  \n"
     ]
    }
   ],
   "source": [
    "#Une los 3 Data Frames\n",
    "# Concatenar los DataFrames horizontalmente por columnas (axis=1)\n",
    "df_final = pd.concat([df_precios, df_title, df_URL], axis=1)\n",
    "\n",
    "# Mostrar el DataFrame final\n",
    "print(df_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punto extra\n",
    "Realiza la misma tarea que arriba pero considera todos los libros de la página\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/carla-andrea06/selenium-web-scraping/tasks/CA_035/tarea_webscraping.ipynb Cell 22\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/carla-andrea06/selenium-web-scraping/tasks/CA_035/tarea_webscraping.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# Agrega las URLs a la lista principal\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/carla-andrea06/selenium-web-scraping/tasks/CA_035/tarea_webscraping.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m urls\u001b[39m.\u001b[39mextend(url_de_libros)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/carla-andrea06/selenium-web-scraping/tasks/CA_035/tarea_webscraping.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m next_button \u001b[39m=\u001b[39m wait\u001b[39m.\u001b[39;49muntil(EC\u001b[39m.\u001b[39;49mpresence_of_element_located((By\u001b[39m.\u001b[39;49mXPATH, \u001b[39m\"\u001b[39;49m\u001b[39m//li[@class=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mnext\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m]/a\u001b[39;49m\u001b[39m\"\u001b[39;49m)))\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/carla-andrea06/selenium-web-scraping/tasks/CA_035/tarea_webscraping.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mdisabled\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m next_button\u001b[39m.\u001b[39mget_attribute(\u001b[39m'\u001b[39m\u001b[39mclass\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/carla-andrea06/selenium-web-scraping/tasks/CA_035/tarea_webscraping.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39mbreak\u001b[39;00m  \u001b[39m# No hay más páginas\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/selenium/webdriver/support/wait.py:92\u001b[0m, in \u001b[0;36mWebDriverWait.until\u001b[0;34m(self, method, message)\u001b[0m\n\u001b[1;32m     90\u001b[0m     screen \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(exc, \u001b[39m\"\u001b[39m\u001b[39mscreen\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     91\u001b[0m     stacktrace \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(exc, \u001b[39m\"\u001b[39m\u001b[39mstacktrace\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m---> 92\u001b[0m time\u001b[39m.\u001b[39;49msleep(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll)\n\u001b[1;32m     93\u001b[0m \u001b[39mif\u001b[39;00m time\u001b[39m.\u001b[39mmonotonic() \u001b[39m>\u001b[39m end_time:\n\u001b[1;32m     94\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "# Inicializa listas para almacenar los datos\n",
    "titulos = []\n",
    "precios = []\n",
    "urls = []\n",
    "#Itera sobre las páginas\n",
    "for _ in range(50):\n",
    "    try:\n",
    "        # Obtén la información de la página actual\n",
    "        url_de_libros = [url.get_attribute(\"href\") for url in driver.find_elements(By.XPATH, xpath_url_de_libros)]\n",
    "        titulos.extend([title.get_attribute(\"title\") for title in driver.find_elements(By.XPATH, xpath_titulos)])\n",
    "        precios.extend([price.text.strip('£') for price in driver.find_elements(By.XPATH, xpath_precios)])\n",
    "        # Agrega las URLs a la lista principal\n",
    "        urls.extend(url_de_libros)\n",
    "        next_button = wait.until(EC.presence_of_element_located((By.XPATH, \"//li[@class='next']/a\")))\n",
    "        if 'disabled' in next_button.get_attribute('class'):\n",
    "            break  # No hay más páginas\n",
    "        else:\n",
    "            # Avanza a la página siguiente\n",
    "            next_button.click()\n",
    "            wait.until(EC.staleness_of(next_button))  # Espera a que la página se cargue completamente\n",
    "    except (NoSuchElementException, StaleElementReferenceException, TimeoutException):\n",
    "         # Si hay algún error al buscar elementos, intenta nuevamente la misma página\n",
    "        pass\n",
    "\n",
    "# Crea el DataFrame de Pandas\n",
    "data = {'Título': titulos, 'Precio (£)': precios, 'URL': url_de_libros}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Muestra el DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "# Inicializa listas para almacenar los datos\n",
    "titulos = []\n",
    "precios = []\n",
    "urls = []\n",
    "\n",
    "# Itera sobre las páginas\n",
    "for _ in range(50):\n",
    "    try:\n",
    "        # Obtén la información de la página actual\n",
    "        url_de_libros = [url.get_attribute(\"href\") for url in driver.find_elements(By.XPATH, xpath_url_de_libros)]\n",
    "        titulos.extend([title.get_attribute(\"title\") for title in driver.find_elements(By.XPATH, xpath_titulos)])\n",
    "        precios.extend([price.text.strip('£') for price in driver.find_elements(By.XPATH, xpath_precios)])\n",
    "        # Agrega las URLs a la lista principal\n",
    "        urls.extend(url_de_libros)\n",
    "\n",
    "        # Verifica si hay una página siguiente\n",
    "        next_button = driver.find_element(By.XPATH, \"//li[@class='next']/a\")\n",
    "        if 'disabled' in next_button.get_attribute('class'):\n",
    "            break  # No hay más páginas\n",
    "        else:\n",
    "            # Avanza a la página siguiente\n",
    "            next_button.click()\n",
    "            wait.until(EC.staleness_of(next_button))  # Espera a que la página se cargue completamente\n",
    "    except NoSuchElementException:\n",
    "        break  # No se encontró el botón \"Next\"\n",
    "\n",
    "# Crea el DataFrame final\n",
    "data = {'Título': titulos, 'Precio (£)': precios, 'URL': urls}\n",
    "df_final = pd.DataFrame(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Título Precio (£)  \\\n",
      "0                                        Frankenstein      38.00   \n",
      "1                    Forever Rockers (The Rocker #12)      28.80   \n",
      "2                         Fighting Fate (Fighting #6)      39.24   \n",
      "3                                                Emma      32.93   \n",
      "4                                     Eat, Pray, Love      51.32   \n",
      "5                     Deep Under (Walker Security #1)      47.09   \n",
      "6   Choosing Our Religion: The Spiritual Lives of ...      28.42   \n",
      "7   Charlie and the Chocolate Factory (Charlie Buc...      22.85   \n",
      "8           Charity's Cross (Charles Towne Belles #4)      41.24   \n",
      "9                                        Bright Lines      39.07   \n",
      "10           Bridget Jones's Diary (Bridget Jones #1)      29.82   \n",
      "11                      Bounty (Colorado Mountain #7)      37.26   \n",
      "12               Blood Defense (Samantha Brinkman #1)      20.30   \n",
      "13  Bleach, Vol. 1: Strawberry and the Soul Reaper...      34.65   \n",
      "14                               Beyond Good and Evil      43.38   \n",
      "15  Alice in Wonderland (Alice's Adventures in Won...      55.53   \n",
      "16   Ajin: Demi-Human, Volume 1 (Ajin: Demi-Human #1)      57.06   \n",
      "17  A Spy's Devotion (The Regency Spies of London #1)      16.97   \n",
      "18                1st to Die (Women's Murder Club #1)      53.98   \n",
      "19                 1,000 Places to See Before You Die      26.08   \n",
      "\n",
      "                                                  URL  \n",
      "0   https://books.toscrape.com/catalogue/frankenst...  \n",
      "1   https://books.toscrape.com/catalogue/forever-r...  \n",
      "2   https://books.toscrape.com/catalogue/fighting-...  \n",
      "3   https://books.toscrape.com/catalogue/emma_17/i...  \n",
      "4   https://books.toscrape.com/catalogue/eat-pray-...  \n",
      "5   https://books.toscrape.com/catalogue/deep-unde...  \n",
      "6   https://books.toscrape.com/catalogue/choosing-...  \n",
      "7   https://books.toscrape.com/catalogue/charlie-a...  \n",
      "8   https://books.toscrape.com/catalogue/charitys-...  \n",
      "9   https://books.toscrape.com/catalogue/bright-li...  \n",
      "10  https://books.toscrape.com/catalogue/bridget-j...  \n",
      "11  https://books.toscrape.com/catalogue/bounty-co...  \n",
      "12  https://books.toscrape.com/catalogue/blood-def...  \n",
      "13  https://books.toscrape.com/catalogue/bleach-vo...  \n",
      "14  https://books.toscrape.com/catalogue/beyond-go...  \n",
      "15  https://books.toscrape.com/catalogue/alice-in-...  \n",
      "16  https://books.toscrape.com/catalogue/ajin-demi...  \n",
      "17  https://books.toscrape.com/catalogue/a-spys-de...  \n",
      "18  https://books.toscrape.com/catalogue/1st-to-di...  \n",
      "19  https://books.toscrape.com/catalogue/1000-plac...  \n"
     ]
    }
   ],
   "source": [
    "print(df_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Help\n",
    "\n",
    "En caso de atorarte, en este video podrás encontrar la manera en la que se pueden hacer las consultas a ChatGTP para obtener dichos XPATHS:https://youtu.be/4YokIEUeSSY?si=OhBi5eum-joIXtkc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
